{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run experiments on MNIST/FashionMNIST",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dfccf3e89c5e48f78dbf28ad805b61fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8e33bf64cd0e4d2d98941e0f0275fc24",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09781824cedf4b42a66e01c71fa81193",
              "IPY_MODEL_3dd98068324045c799882059450f86c3"
            ]
          }
        },
        "8e33bf64cd0e4d2d98941e0f0275fc24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09781824cedf4b42a66e01c71fa81193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_030207cfadba4ef2a5c60504b76129b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0ea5542504a4115baf1177cb52829c2"
          }
        },
        "3dd98068324045c799882059450f86c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f3836cff1fff4c009fe020c4c5db6415",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26427392/? [00:20&lt;00:00, 6519212.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3d99c89e39f40f39a6e9af6994c2f78"
          }
        },
        "030207cfadba4ef2a5c60504b76129b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0ea5542504a4115baf1177cb52829c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3836cff1fff4c009fe020c4c5db6415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3d99c89e39f40f39a6e9af6994c2f78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aec2134630f44df28903c79479ca0589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7858d1bc512b4858b9d5b7d483cc6190",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_04afe340d0384819b7a0e921fda29306",
              "IPY_MODEL_db288bf9fdc04a7ba4a17c0d2753f4c1"
            ]
          }
        },
        "7858d1bc512b4858b9d5b7d483cc6190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04afe340d0384819b7a0e921fda29306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_648128391d264fcf8b73f595ad181916",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e66dd1c3d27c4318952324d8b8c7ff30"
          }
        },
        "db288bf9fdc04a7ba4a17c0d2753f4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b513d76c07b64f969c7477971d5a5c4e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:00&lt;00:00, 64467.43it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c2807ba2c154e93903d6641af445714"
          }
        },
        "648128391d264fcf8b73f595ad181916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e66dd1c3d27c4318952324d8b8c7ff30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b513d76c07b64f969c7477971d5a5c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c2807ba2c154e93903d6641af445714": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4511f97522a34cb9b1a987ef8baf444d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8a355c168662472fa78dbb82b2da7f9a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61e0b1f23a2146e39330c6ff239c74a7",
              "IPY_MODEL_9891dc7983b140ba9c6acd6f94bc748b"
            ]
          }
        },
        "8a355c168662472fa78dbb82b2da7f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61e0b1f23a2146e39330c6ff239c74a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_04eec0229cfc420980834c43a87cd996",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3388ec542ea64e8c8890635808a62beb"
          }
        },
        "9891dc7983b140ba9c6acd6f94bc748b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c7833054c33b4292a3e03602f80921ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4423680/? [00:16&lt;00:00, 798441.08it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eaf9eb1c701a43e2bd5e49cac250ff71"
          }
        },
        "04eec0229cfc420980834c43a87cd996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3388ec542ea64e8c8890635808a62beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7833054c33b4292a3e03602f80921ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eaf9eb1c701a43e2bd5e49cac250ff71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ceba0c9fb6544a0380bd6e9705179eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c6ca8194cb254960ba42f4f6c90851a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9ecb94f9a4864b1497870728bd58fb36",
              "IPY_MODEL_460be2634f574eb19fed39bf6703394a"
            ]
          }
        },
        "c6ca8194cb254960ba42f4f6c90851a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ecb94f9a4864b1497870728bd58fb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_24fcc7830ad94dddbe0eb78ef0a181eb",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bea6c4ed9d0d42e9b3e62e7d67ffb300"
          }
        },
        "460be2634f574eb19fed39bf6703394a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4cd338ecc21148fa8ad795f766668ec8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/5148 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5bd7c8bd8a34a07aab01ae5d0cb4728"
          }
        },
        "24fcc7830ad94dddbe0eb78ef0a181eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bea6c4ed9d0d42e9b3e62e7d67ffb300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4cd338ecc21148fa8ad795f766668ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5bd7c8bd8a34a07aab01ae5d0cb4728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnegebert/vae-iwae-vralpha/blob/master/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPkcLUJMLpNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim, Tensor as T\n",
        "from torch.autograd import detect_anomaly\n",
        "from torch.distributions.multinomial import Multinomial\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4jivVFmwX7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "alpha = -500 # alpha value used in Renyi alpha-divergence, ignored when model_type is vae/iwae/vrmax\n",
        "K = 5 # number of samples taken per input data point\n",
        "L = 2 # number of stochastic layers in network architecture; either 1 or 2\n",
        "\n",
        "algorithm = 'vralpha' # one of ['vae', 'iwae', 'vrmax', 'vralpha', 'general_alpha']\n",
        "# For details for each of these, especially VR-max, VR-alpha and general alpha, please see the\n",
        "# Renyi Divergence Variational Inference paper\n",
        "# But shortly:\n",
        "# vae\n",
        "#   = Use L_VI objective (Optimize (1/K)*SUM(log[ p(x,z_k)/q(z_k|x)])).\n",
        "#   Choice of alpha is ignored.\n",
        "# iwae\n",
        "#   = Use IWAE objective L_k (Optimize log[(1/K)*SUM(p(x,z)/q(z|x))]).\n",
        "#   Choice of alpha ignored.\n",
        "# vrmax\n",
        "#   = Use VR-max algorithm.\n",
        "#   => (Optimize log[ MAX_k(p(x,z_k)/q(z_k|x))] ).\n",
        "#   Choice of alpha is ignored.\n",
        "# vralpha\n",
        "#   = Use VR-alpha\n",
        "#   => (Optimize log[ p(x,z_k)/q(z_k|x)] where the kth sample is whosen w.p. ~ magnitude^(1-alpha)).\n",
        "#   Choice of alpha is important.\n",
        "# general_alpha\n",
        "#   = Use direct estimate of L_{\\alpha,K}\n",
        "#   => (Optimize [1/(1-alpha)]*log((1/K)*SUM( log[ (p(x,z_k)/q(z_k|x))^(1-alpha) ] ))).\n",
        "#   Choice of alpha is important.\n",
        "#   (general_alpha is the same as VR-alpha except that we backpropagate K samples instead of only one.\n",
        "#   That is, we don't estimate the objective by taking only one of the K samples, but instead utilize all\n",
        "#   K samples. )\n",
        "\n",
        "data_name = 'fashion' # one of ['mnist', 'fashion', 'fashionmnist']\n",
        "\n",
        "epochs = 100\n",
        "learning_rate = 1e-3\n",
        "\n",
        "log_interval = 1 # how frequently to log average training loss\n",
        "test_interval = 5 # how frequently to test\n",
        "train_batch_size = 256 # batch size during training\n",
        "test_batch_size = 32 # batch size used during testing, different than training because testing is done with K=5000\n",
        "\n",
        "seed = 0 # fixed seed\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "os.makedirs('results', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "assert(L in [1, 2]) # we only have networks with 1 or 2 stochastic layers\n",
        "assert(algorithm in ['vae', 'iwae', 'vrmax', 'vralpha', 'general_alpha'])\n",
        "assert(not(alpha==1 and algorithm in ['vralpha', 'general_alpha'])) # divide by 0 error otherwise\n",
        "assert(data_name in ['mnist', 'fashion', 'fashionmnist'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "L0klQrhtrdQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_and_initialize_loaders(data_name, train_batch, test_batch):\n",
        "    data_name = data_name.lower()\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "    if data_name == 'mnist':\n",
        "        train_data = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "        test_data = datasets.MNIST('./data', train=False, transform=transforms.ToTensor())\n",
        "    elif data_name == 'fashion' or data_name == 'fashionmnist':\n",
        "        train_data = datasets.FashionMNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "        test_data = datasets.FashionMNIST('./data', train=False, transform=transforms.ToTensor())\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = train_batch, shuffle = True, ** kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = test_batch, shuffle = True, ** kwargs)\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eNxKyqfkrdQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implementation of the VAE used on the MNIST dataset with 1 stochastic layer.\n",
        "class mnist_model_1(nn.Module):\n",
        "    def __init__(self, alpha):\n",
        "        super(mnist_model_1, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.fc31 = nn.Linear(200, 50)\n",
        "        self.fc32 = nn.Linear(200, 50)\n",
        "\n",
        "        self.fc4 = nn.Linear(50, 200)\n",
        "        self.fc5 = nn.Linear(200, 200)\n",
        "        self.fc6 = nn.Linear(200, 784)\n",
        "\n",
        "        self.K = K\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = torch.tanh(self.fc1(x))\n",
        "        h2 = torch.tanh(self.fc2(h1))\n",
        "        return self.fc31(h2), self.fc32(h2)\n",
        "\n",
        "    def reparameterize(self, mu, logstd):\n",
        "        std = torch.exp(logstd)\n",
        "        eps = torch.randn_like(std)\n",
        "        # This is the reparametrization trick - represent the sample as a sum rather than black-box generated number\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = torch.tanh(self.fc4(z))\n",
        "        h4 = torch.tanh(self.fc5(h3))\n",
        "        return torch.sigmoid(self.fc6(h4))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logstd = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logstd)\n",
        "        return self.decode(z), mu, logstd\n",
        "\n",
        "    def compute_loss_for_batch(self, data, model, K=K, test=False):\n",
        "        # data = (B, 1, H, W)\n",
        "        B, _, H, W = data.shape\n",
        "\n",
        "        # Generate K copies of each observation. Each will get sampled once according to the generated distribution to generate a total of K observation samples\n",
        "        data_k_vec = data.repeat((1, K, 1, 1)).view(-1, H*W)\n",
        "\n",
        "        # Retrieve the estimated mean and log(standard deviation) estimates from the posterior approximator\n",
        "        mu, logstd = model.encode(data_k_vec)\n",
        "\n",
        "        # Use the reparametrization trick to generate (mean)+(epsilon)*(standard deviation) for each sample of each observation\n",
        "        z = model.reparameterize(mu, logstd)\n",
        "\n",
        "        # Calculate log q(z|x) - how likely are the importance samples given the distribution that generated them?\n",
        "        log_q = compute_log_probabitility_gaussian(z, mu, logstd)\n",
        "\n",
        "        # Calculate log p(z) - how likely are the importance samples under the prior N(0,1) assumption?\n",
        "        log_p_z = compute_log_probabitility_gaussian(z, torch.zeros_like(z, requires_grad=False), torch.zeros_like(z, requires_grad=False))\n",
        "\n",
        "        # Hand the samples to the decoder network and get a reconstruction of each sample.\n",
        "        decoded = model.decode(z)\n",
        "\n",
        "        # Calculate log p(x|z) with a bernoulli distribution - how likely are the recreations given the latents that generated them?\n",
        "        log_p = compute_log_probabitility_bernoulli(decoded, data_k_vec)\n",
        "\n",
        "        # Begin calculating L_alpha depending on the (a) model type, and (b) optimization method\n",
        "        # log_p_z + log_p - log_q = log(p(z_i)p(x|z_i)/q(z_i|x)) = log(p(x,z_i)/q(z_i|x)) = L_VI\n",
        "        #   (for each importance sample i out of K for each observation)\n",
        "        if algorithm == 'iwae' or test:\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            log_w_matrix = (log_p_z + log_p - log_q).view(B, K)\n",
        "\n",
        "        elif algorithm =='vae':\n",
        "            # Don't reorder, and divide by K in anticipation of taking a batch sum of (1/K)*SUM(log(p(x,z)/q(z|x)))\n",
        "            log_w_matrix = (log_p_z + log_p - log_q).view(B*K, 1)*1/K\n",
        "\n",
        "        elif algorithm=='general_alpha' or algorithm=='vralpha':\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            # Multiply by (1-alpha) because (1-alpha)* log(p(x,z_i)/q(z_i|x)) =  log([p(x,z_i)/q(z_i|x)]^(1-alpha))\n",
        "            log_w_matrix = (log_p_z + log_p - log_q).view(-1, K) * (1-alpha)\n",
        "\n",
        "        elif algorithm == 'vrmax':\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            # Take the max in each row, representing the maximum-weighted sample\n",
        "            log_w_matrix = (log_p_z + log_p - log_q).view(-1, K).max(axis=1, keepdim=True).values\n",
        "\n",
        "            # immediately return loss = -sum(L_alpha) over each observation\n",
        "            return -torch.sum(log_w_matrix)\n",
        "\n",
        "        # Begin using the \"max trick\". Subtract the maximum log(*) sample value for each observation.\n",
        "        # log_w_minus_max = log([p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]))\n",
        "        log_w_minus_max = log_w_matrix - torch.max(log_w_matrix, 1, keepdim=True)[0]\n",
        "\n",
        "        # Exponentiate so that each term is [p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]) (no log)\n",
        "        ws_matrix = torch.exp(log_w_minus_max)\n",
        "\n",
        "        # Calculate normalized weights in each row. Max denominators cancel out!\n",
        "        # ws_norm = [p(z_i,x)/q(z_i|x)]/SUM([p(z_k,x)/q(z_k|x)])\n",
        "        ws_norm = ws_matrix / torch.sum(ws_matrix, 1, keepdim=True)\n",
        "\n",
        "        if algorithm == 'vralpha' and not test:\n",
        "            # If we're specifically using a VR-alpha model, we want to choose a sample to backprop according to the values in ws_norm above\n",
        "            # So we make a distribution in each row\n",
        "            sample_dist = Multinomial(1, ws_norm)\n",
        "\n",
        "            # Then we choose a sample in each row acccording to this distribution\n",
        "            ws_sum_per_datapoint = log_w_matrix.gather(1, sample_dist.sample().argmax(1, keepdim=True))\n",
        "        else:\n",
        "            # For any other model, we're taking the full sum at this point\n",
        "            ws_sum_per_datapoint = torch.sum(log_w_matrix * ws_norm, 1)\n",
        "\n",
        "        if algorithm in [\"general_alpha\", \"vralpha\"] and not test:\n",
        "            # For both VR-alpha and directly estimating L_alpha with a sum, we have to renormalize the sum with 1-alpha\n",
        "            ws_sum_per_datapoint /= (1 - alpha)\n",
        "\n",
        "        # Return a value of loss = -L_alpha as the batch sum.\n",
        "        loss = -torch.sum(ws_sum_per_datapoint)\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-b0DLZyfrdQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implementation of the VAE used on the MNIST dataset with 2 stochastic layers.\n",
        "class mnist_model_2(nn.Module):\n",
        "    def __init__(self, alpha):\n",
        "        super(mnist_model_2, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.fc31 = nn.Linear(200, 100)  # stochastic 1\n",
        "        self.fc32 = nn.Linear(200, 100)\n",
        "\n",
        "        self.fc4 = nn.Linear(100, 100)\n",
        "        self.fc5 = nn.Linear(100, 100)\n",
        "        self.fc61 = nn.Linear(100, 50)  # Innermost (stochastic 2)\n",
        "        self.fc62 = nn.Linear(100, 50)\n",
        "\n",
        "        self.fc7 = nn.Linear(50, 100)\n",
        "        self.fc8 = nn.Linear(100, 100)\n",
        "        self.fc81 = nn.Linear(100, 100)  # stochastic 1\n",
        "        self.fc82 = nn.Linear(100, 100)\n",
        "\n",
        "        self.fc9 = nn.Linear(100, 200)\n",
        "        self.fc10 = nn.Linear(200, 200)\n",
        "        self.fc11 = nn.Linear(200, 784)  # reconstruction\n",
        "\n",
        "        self.K = K\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = torch.tanh(self.fc1(x))\n",
        "        h2 = torch.tanh(self.fc2(h1))\n",
        "        mu, log_std = self.fc31(h2), self.fc32(h2)\n",
        "\n",
        "        z1 = self.reparameterize(mu, log_std)\n",
        "        h3 = torch.tanh(self.fc4(z1))\n",
        "        h4 = torch.tanh(self.fc5(h3))\n",
        "\n",
        "        return self.fc61(h4), self.fc62(h4), [x, z1]\n",
        "\n",
        "    def reparameterize(self, mu, logstd, test=False):\n",
        "        std = torch.exp(logstd)\n",
        "        if test == True:\n",
        "            eps = torch.zeros_like(mu)\n",
        "        else:\n",
        "            eps = torch.randn_like(std)\n",
        "        # This is the reparametrization trick - represent the sample as a sum rather than black-box generated number\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, test=False):\n",
        "        h5 = torch.tanh(self.fc7(z))\n",
        "        h6 = torch.tanh(self.fc8(h5))\n",
        "        mu, log_std = self.fc81(h6), self.fc82(h6)\n",
        "\n",
        "        z1 = self.reparameterize(mu, log_std, test=test)\n",
        "        h7 = torch.tanh(self.fc9(z1))\n",
        "        h8 = torch.tanh(self.fc10(h7))\n",
        "\n",
        "        return torch.sigmoid(self.fc11(h8))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logstd, _ = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logstd)\n",
        "        return self.decode(z), mu, logstd\n",
        "\n",
        "    def compute_loss_for_batch(self, data, model, K=K, test=False):\n",
        "        B, _, H, W = data.shape\n",
        "\n",
        "        # First repeat the observations K times, representing the data as a flat (M*K, # of pixels)\n",
        "        data_k_vec = data.repeat((1, K, 1, 1)).view(-1, H * W)\n",
        "\n",
        "        # Encode the model and retrieve estimated distribution parameters mu and log(standard deviation) for each sample of each observation\n",
        "        # z1 holds the latent samples generated at the first stochastic layer.\n",
        "        mu, log_std, [x, z1] = self.encode(data_k_vec)\n",
        "\n",
        "        # Sample from each observation's approximated latent distribution in each row (i.e. once for each of K importance samples, represented by rows)\n",
        "        # (this uses the reparametrization trick!)\n",
        "        z = model.reparameterize(mu, log_std)\n",
        "\n",
        "        # Calculate Log p(z) (prior) - how likely are these values given the prior assumption N(0,1)?\n",
        "        log_p_z = torch.sum(-0.5 * z ** 2, 1) - .5 * z.shape[1] * T.log(torch.tensor(2 * np.pi))\n",
        "\n",
        "        # Calculate q (z | h1) - how likely are the generated output latent samples given the distributions they came from?\n",
        "        log_qz_h1 = compute_log_probabitility_gaussian(z, mu, log_std)\n",
        "\n",
        "        # Re-Generate the mu and log_std that generated the first-layer latents z1\n",
        "        h1 = torch.tanh(self.fc1(x))\n",
        "        h2 = torch.tanh(self.fc2(h1))\n",
        "        mu, log_std = self.fc31(h2), self.fc32(h2)\n",
        "\n",
        "        # Calculate log q(h1|x) - how likely are the first-stochastic-layer latents given the distributions they come from?\n",
        "        log_qh1_x = compute_log_probabitility_gaussian(z1, mu, log_std)\n",
        "\n",
        "        # Calculate the distribution parameters that generated the first-layer latents upon decoding\n",
        "        h5 = torch.tanh(self.fc7(z))\n",
        "        h6 = torch.tanh(self.fc8(h5))\n",
        "        mu, log_std = self.fc81(h6), self.fc82(h6)\n",
        "\n",
        "        # Calculate log p(h1|z) - how likely are the latents z1 under the parameters of the distribution here?\n",
        "        #   (This directly encourages the decoder to learn the inverse of the map h1->z)\n",
        "        log_ph1_z = compute_log_probabitility_gaussian(z1, mu, log_std)\n",
        "\n",
        "        # Finally calculate the reconstructed image\n",
        "        h7 = torch.tanh(self.fc9(z1))\n",
        "        h8 = torch.tanh(self.fc10(h7))\n",
        "        decoded = torch.sigmoid(self.fc11(h8))\n",
        "\n",
        "        # calculate log p(x | h1) - how likely is the reconstruction given the latent samples that generated it?\n",
        "        log_px_h1 = compute_log_probabitility_bernoulli(decoded, x)\n",
        "\n",
        "        # Begin calculating L_alpha depending on the (a) model type, and (b) optimization method\n",
        "        # log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x =\n",
        "        #           log([p(z0_i)p(x|z1_i)p(z1_i|z0_i)]/[q(z0_i|z1_i)q(z1_i|x)]) = log(p(x,z0_i,z1_i)/q(z0_i,z1_i|x)) = L_VI\n",
        "        #   (for each importance sample i out of K for each observation)\n",
        "        # Note that if test==True then we're always using the IWAE objective!\n",
        "        if algorithm == 'iwae' or test == True:\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, K)\n",
        "\n",
        "        elif algorithm == 'vae':\n",
        "            # Don't reorder, and divide by K in anticipation of taking a batch sum of (1/K)*SUM(log(p(x,z)/q(z|x)))\n",
        "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, 1) * 1 / K\n",
        "            return -torch.sum(log_w_matrix)\n",
        "\n",
        "        elif algorithm == 'general_alpha' or algorithm == 'vralpha':\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            # Multiply by (1-alpha) because (1-alpha)* log(p(x,z_i)/q(z_i|x)) =  log([p(x,z_i)/q(z_i|x)]^(1-alpha))\n",
        "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, K) * (1 - self.alpha)\n",
        "\n",
        "        elif algorithm == 'vrmax':\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            # Take the max in each row, representing the maximum-weighted sample, then immediately return batch sum loss -L_alpha\n",
        "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, K).max(axis=1,keepdim=True).values\n",
        "            return -torch.sum(log_w_matrix)\n",
        "\n",
        "        # Begin using the \"max trick\". Subtract the maximum log(*) sample value for each observation.\n",
        "        # log_w_minus_max = log([p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]))\n",
        "        log_w_minus_max = log_w_matrix - torch.max(log_w_matrix, 1, keepdim=True)[0]\n",
        "\n",
        "        # Exponentiate so that each term is [p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]) (no log)\n",
        "        ws_matrix = torch.exp(log_w_minus_max)\n",
        "\n",
        "        # Calculate normalized weights in each row. Max denominators cancel out!\n",
        "        # ws_norm = [p(z_i,x)/q(z_i|x)]/SUM([p(z_k,x)/q(z_k|x)])\n",
        "        ws_norm = ws_matrix / torch.sum(ws_matrix, 1, keepdim=True)\n",
        "\n",
        "        if algorithm == 'vralpha' and not test:\n",
        "            # If we're specifically using a VR-alpha model, we want to choose a sample to backprop according to the values in ws_norm above\n",
        "            # So we make a distribution in each row\n",
        "            sample_dist = Multinomial(1, ws_norm)\n",
        "\n",
        "            # Then we choose a sample in each row acccording to this distribution\n",
        "            ws_sum_per_datapoint = log_w_matrix.gather(1, sample_dist.sample().argmax(1, keepdim=True))\n",
        "        else:\n",
        "            # For any other model, we're taking the full sum at this point\n",
        "            ws_sum_per_datapoint = torch.sum(log_w_matrix * ws_norm, 1)\n",
        "\n",
        "        if algorithm in [\"general_alpha\", \"vralpha\"] and not test:\n",
        "            # For both VR-alpha and directly estimating L_alpha with a sum, we have to renormalize the sum with 1-alpha\n",
        "            ws_sum_per_datapoint /= (1 - alpha)\n",
        "\n",
        "        loss = -torch.sum(ws_sum_per_datapoint)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KVDo9bU_rdQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute N(obs| mu, sigma) for all K samples and sum over probabilities of the K samples\n",
        "def compute_log_probabitility_gaussian(obs, mu, logstd, axis=1):\n",
        "    return torch.sum(-0.5 * ((obs-mu) / torch.exp(logstd)) ** 2 - logstd, axis)-.5*obs.shape[1]*T.log(torch.tensor(2*np.pi))\n",
        "\n",
        "# Compute Ber(obs| theta) for all K samples and sum over probabilities of the K samples\n",
        "def compute_log_probabitility_bernoulli(theta, obs, axis=1):\n",
        "    # 1e-18 needed to avoid numerical errors\n",
        "    return torch.sum(obs*torch.log(theta+1e-18) + (1-obs)*torch.log(1-theta+1e-18), axis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "khnN29nKrdQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train and test functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        # (B, 1, F1, F2) (e.g. (128, 1, 28, 28) for MNIST with B=128)\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = model.compute_loss_for_batch(data, model)\n",
        "        # comment this back in in case of NaNs\n",
        "        #with detect_anomaly():\n",
        "        #    loss.backward()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % log_interval == 0:\n",
        "        print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
        "        logging.info(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, labels) in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss = model.compute_loss_for_batch(data, model, K=5000,test=True)\n",
        "            test_loss += loss.item()\n",
        "            if i == 0:\n",
        "                # Visualizing reconstructions\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                      recon_batch.view(test_batch_size, 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(),\n",
        "                         f'results/reconstruction_{algorithm}_L={L}_{data_name}_alpha={alpha}_K={K}_epoch={epoch}.png',\n",
        "                           nrow=n)\n",
        "                # Visualizing random samples from the latent space\n",
        "                noise = torch.randn(64, 50).to(device)\n",
        "                sample = model.decode(noise).cpu()\n",
        "                save_image(sample.view(64, 1, 28, 28),\n",
        "                           f'results/sample_{algorithm}_L={L}_{data_name}_alpha={alpha}_K={K}_epoch={epoch}.png')\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'====> Epoch: {epoch} Test set loss: {test_loss:.4f}')\n",
        "    logging.info(f'====> Epoch: {epoch} Test set loss: {test_loss:.4f}')\n",
        "    return test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "b-wqHyVxrdQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "dfccf3e89c5e48f78dbf28ad805b61fb",
            "8e33bf64cd0e4d2d98941e0f0275fc24",
            "09781824cedf4b42a66e01c71fa81193",
            "3dd98068324045c799882059450f86c3",
            "030207cfadba4ef2a5c60504b76129b0",
            "e0ea5542504a4115baf1177cb52829c2",
            "f3836cff1fff4c009fe020c4c5db6415",
            "d3d99c89e39f40f39a6e9af6994c2f78",
            "aec2134630f44df28903c79479ca0589",
            "7858d1bc512b4858b9d5b7d483cc6190",
            "04afe340d0384819b7a0e921fda29306",
            "db288bf9fdc04a7ba4a17c0d2753f4c1",
            "648128391d264fcf8b73f595ad181916",
            "e66dd1c3d27c4318952324d8b8c7ff30",
            "b513d76c07b64f969c7477971d5a5c4e",
            "6c2807ba2c154e93903d6641af445714",
            "4511f97522a34cb9b1a987ef8baf444d",
            "8a355c168662472fa78dbb82b2da7f9a",
            "61e0b1f23a2146e39330c6ff239c74a7",
            "9891dc7983b140ba9c6acd6f94bc748b",
            "04eec0229cfc420980834c43a87cd996",
            "3388ec542ea64e8c8890635808a62beb",
            "c7833054c33b4292a3e03602f80921ce",
            "eaf9eb1c701a43e2bd5e49cac250ff71",
            "ceba0c9fb6544a0380bd6e9705179eeb",
            "c6ca8194cb254960ba42f4f6c90851a6",
            "9ecb94f9a4864b1497870728bd58fb36",
            "460be2634f574eb19fed39bf6703394a",
            "24fcc7830ad94dddbe0eb78ef0a181eb",
            "bea6c4ed9d0d42e9b3e62e7d67ffb300",
            "4cd338ecc21148fa8ad795f766668ec8",
            "e5bd7c8bd8a34a07aab01ae5d0cb4728"
          ]
        },
        "outputId": "21eeb090-28ad-459d-a66a-09e9530960e3"
      },
      "source": [
        "if L==1:\n",
        "    model = mnist_model_1(alpha).to(device)\n",
        "else:\n",
        "    model = mnist_model_2(alpha).to(device)\n",
        "train_loader, test_loader = load_data_and_initialize_loaders(data_name, train_batch_size, test_batch_size)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfccf3e89c5e48f78dbf28ad805b61fb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aec2134630f44df28903c79479ca0589",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4511f97522a34cb9b1a987ef8baf444d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceba0c9fb6544a0380bd6e9705179eeb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "Training on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "IEaIzm1crdQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5d374c62-6d82-453e-dcbd-3992b099a3b3"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Training on GPU\")\n",
        "    logging.info(\"Training on GPU\")\n",
        "\n",
        "print(f'{datetime.datetime.now()} \\nStarting training')\n",
        "logging.info(f'{datetime.datetime.now()} \\nStarting training')\n",
        "for e in range(1, epochs+1):\n",
        "    train(e)\n",
        "    if e % test_interval == 0:\n",
        "        test(e)\n",
        "test(epochs)\n",
        "print(datetime.datetime.now())\n",
        "logging.info(datetime.datetime.now())\n",
        "print(\"Training finished\")\n",
        "logging.info(\"Training finished\")\n",
        "\n",
        "print(\"Saving model\")\n",
        "torch.save(model.state_dict(),\n",
        "           f'models/{algorithm}_L={L}_{data_name}_alpha={alpha}_K={K}_epochs={epochs}.pt')\n",
        "print(\"Saved model\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU\n",
            "2020-06-12 18:12:56.008404 \n",
            "Starting training\n",
            "====> Epoch: 1 Average loss: 237.5309\n",
            "====> Epoch: 1 Test set loss: 234.6304\n",
            "2020-06-12 18:13:33.955103\n",
            "Training finished\n",
            "Saving model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}