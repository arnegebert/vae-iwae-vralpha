{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Run experiments on MNIST/FashionMNIST",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "dfccf3e89c5e48f78dbf28ad805b61fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_8e33bf64cd0e4d2d98941e0f0275fc24",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_09781824cedf4b42a66e01c71fa81193",
       "IPY_MODEL_3dd98068324045c799882059450f86c3"
      ]
     }
    },
    "8e33bf64cd0e4d2d98941e0f0275fc24": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "09781824cedf4b42a66e01c71fa81193": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_030207cfadba4ef2a5c60504b76129b0",
      "_dom_classes": [],
      "description": "",
      "_model_name": "FloatProgressModel",
      "bar_style": "info",
      "max": 1,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 1,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_e0ea5542504a4115baf1177cb52829c2"
     }
    },
    "3dd98068324045c799882059450f86c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_f3836cff1fff4c009fe020c4c5db6415",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 26427392/? [00:20&lt;00:00, 6519212.35it/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_d3d99c89e39f40f39a6e9af6994c2f78"
     }
    },
    "030207cfadba4ef2a5c60504b76129b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "e0ea5542504a4115baf1177cb52829c2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "f3836cff1fff4c009fe020c4c5db6415": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "d3d99c89e39f40f39a6e9af6994c2f78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "aec2134630f44df28903c79479ca0589": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_7858d1bc512b4858b9d5b7d483cc6190",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_04afe340d0384819b7a0e921fda29306",
       "IPY_MODEL_db288bf9fdc04a7ba4a17c0d2753f4c1"
      ]
     }
    },
    "7858d1bc512b4858b9d5b7d483cc6190": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "04afe340d0384819b7a0e921fda29306": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_648128391d264fcf8b73f595ad181916",
      "_dom_classes": [],
      "description": "",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 1,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 1,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_e66dd1c3d27c4318952324d8b8c7ff30"
     }
    },
    "db288bf9fdc04a7ba4a17c0d2753f4c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_b513d76c07b64f969c7477971d5a5c4e",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 32768/? [00:00&lt;00:00, 64467.43it/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_6c2807ba2c154e93903d6641af445714"
     }
    },
    "648128391d264fcf8b73f595ad181916": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "e66dd1c3d27c4318952324d8b8c7ff30": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "b513d76c07b64f969c7477971d5a5c4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "6c2807ba2c154e93903d6641af445714": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "4511f97522a34cb9b1a987ef8baf444d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_8a355c168662472fa78dbb82b2da7f9a",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_61e0b1f23a2146e39330c6ff239c74a7",
       "IPY_MODEL_9891dc7983b140ba9c6acd6f94bc748b"
      ]
     }
    },
    "8a355c168662472fa78dbb82b2da7f9a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "61e0b1f23a2146e39330c6ff239c74a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_04eec0229cfc420980834c43a87cd996",
      "_dom_classes": [],
      "description": "",
      "_model_name": "FloatProgressModel",
      "bar_style": "info",
      "max": 1,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 1,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_3388ec542ea64e8c8890635808a62beb"
     }
    },
    "9891dc7983b140ba9c6acd6f94bc748b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_c7833054c33b4292a3e03602f80921ce",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 4423680/? [00:16&lt;00:00, 798441.08it/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_eaf9eb1c701a43e2bd5e49cac250ff71"
     }
    },
    "04eec0229cfc420980834c43a87cd996": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "3388ec542ea64e8c8890635808a62beb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "c7833054c33b4292a3e03602f80921ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "eaf9eb1c701a43e2bd5e49cac250ff71": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "ceba0c9fb6544a0380bd6e9705179eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_c6ca8194cb254960ba42f4f6c90851a6",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_9ecb94f9a4864b1497870728bd58fb36",
       "IPY_MODEL_460be2634f574eb19fed39bf6703394a"
      ]
     }
    },
    "c6ca8194cb254960ba42f4f6c90851a6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "9ecb94f9a4864b1497870728bd58fb36": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_24fcc7830ad94dddbe0eb78ef0a181eb",
      "_dom_classes": [],
      "description": "  0%",
      "_model_name": "FloatProgressModel",
      "bar_style": "info",
      "max": 1,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 0,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_bea6c4ed9d0d42e9b3e62e7d67ffb300"
     }
    },
    "460be2634f574eb19fed39bf6703394a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_4cd338ecc21148fa8ad795f766668ec8",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 0/5148 [00:00&lt;?, ?it/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_e5bd7c8bd8a34a07aab01ae5d0cb4728"
     }
    },
    "24fcc7830ad94dddbe0eb78ef0a181eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "bea6c4ed9d0d42e9b3e62e7d67ffb300": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "4cd338ecc21148fa8ad795f766668ec8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "e5bd7c8bd8a34a07aab01ae5d0cb4728": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/arnegebert/vae-iwae-vralpha/blob/master/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qPkcLUJMLpNZ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim, Tensor as T\n",
    "from torch.autograd import detect_anomaly\n",
    "from torch.distributions.multinomial import Multinomial\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g4jivVFmwX7W",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Hyperparameters\n",
    "alpha = -500 # alpha value used in Renyi alpha-divergence, ignored when model_type is vae/iwae/vrmax\n",
    "K = 5 # number of samples taken per input data point\n",
    "L = 2 # number of stochastic layers in network architecture; either 1 or 2\n",
    "\n",
    "algorithm = 'vralpha' # one of ['vae', 'iwae', 'vrmax', 'vralpha', 'general_alpha']\n",
    "# For details for each of these, especially VR-max, VR-alpha and general alpha, please see the\n",
    "# Renyi Divergence Variational Inference paper\n",
    "# But shortly:\n",
    "# vae\n",
    "#   = Use L_VI objective (Optimize (1/K)*SUM(log[ p(x,z_k)/q(z_k|x)])).\n",
    "#   Choice of alpha is ignored.\n",
    "# iwae\n",
    "#   = Use IWAE objective L_k (Optimize log[(1/K)*SUM(p(x,z)/q(z|x))]).\n",
    "#   Choice of alpha ignored.\n",
    "# vrmax\n",
    "#   = Use VR-max algorithm.\n",
    "#   => (Optimize log[ MAX_k(p(x,z_k)/q(z_k|x))] ).\n",
    "#   Choice of alpha is ignored.\n",
    "# vralpha\n",
    "#   = Use VR-alpha\n",
    "#   => (Optimize log[ p(x,z_k)/q(z_k|x)] where the kth sample is whosen w.p. ~ magnitude^(1-alpha)).\n",
    "#   Choice of alpha is important.\n",
    "# general_alpha\n",
    "#   = Use direct estimate of L_{\\alpha,K}\n",
    "#   => (Optimize [1/(1-alpha)]*log((1/K)*SUM( log[ (p(x,z_k)/q(z_k|x))^(1-alpha) ] ))).\n",
    "#   Choice of alpha is important.\n",
    "#   (general_alpha is the same as VR-alpha except that we backpropagate K samples instead of only one.\n",
    "#   That is, we don't estimate the objective by taking only one of the K samples, but instead utilize all\n",
    "#   K samples. )\n",
    "\n",
    "data_name = 'fashion' # one of ['mnist', 'fashion', 'fashionmnist']\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_interval = 1 # how frequently to log average training loss\n",
    "test_interval = 5 # how frequently to test\n",
    "train_batch_size = 256 # batch size during training\n",
    "test_batch_size = 32 # batch size used during testing, different than training because testing is done with K=5000\n",
    "\n",
    "seed = 0 # fixed seed\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert(L in [1, 2]) # we only have networks with 1 or 2 stochastic layers\n",
    "assert(algorithm in ['vae', 'iwae', 'vrmax', 'vralpha', 'general_alpha'])\n",
    "assert(not(alpha==1 and algorithm in ['vralpha', 'general_alpha'])) # divide by 0 error otherwise\n",
    "assert(data_name in ['mnist', 'fashion', 'fashionmnist'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "L0klQrhtrdQe",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def load_data_and_initialize_loaders(data_name, train_batch, test_batch):\n",
    "    data_name = data_name.lower()\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    if data_name == 'mnist':\n",
    "        train_data = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "        test_data = datasets.MNIST('./data', train=False, transform=transforms.ToTensor())\n",
    "    elif data_name == 'fashion' or data_name == 'fashionmnist':\n",
    "        train_data = datasets.FashionMNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "        test_data = datasets.FashionMNIST('./data', train=False, transform=transforms.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = train_batch, shuffle = True, ** kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = test_batch, shuffle = True, ** kwargs)\n",
    "    return train_loader, test_loader"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eNxKyqfkrdQh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Implementation of the VAE used on the MNIST dataset with 1 stochastic layer.\n",
    "class mnist_model_1(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(mnist_model_1, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc31 = nn.Linear(200, 50)\n",
    "        self.fc32 = nn.Linear(200, 50)\n",
    "\n",
    "        self.fc4 = nn.Linear(50, 200)\n",
    "        self.fc5 = nn.Linear(200, 200)\n",
    "        self.fc6 = nn.Linear(200, 784)\n",
    "\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.tanh(self.fc1(x))\n",
    "        h2 = torch.tanh(self.fc2(h1))\n",
    "        return self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logstd):\n",
    "        std = torch.exp(logstd)\n",
    "        eps = torch.randn_like(std)\n",
    "        # This is the reparametrization trick - represent the sample as a sum rather than black-box generated number\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.tanh(self.fc4(z))\n",
    "        h4 = torch.tanh(self.fc5(h3))\n",
    "        return torch.sigmoid(self.fc6(h4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logstd = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logstd)\n",
    "        return self.decode(z), mu, logstd\n",
    "\n",
    "    def compute_loss_for_batch(self, data, model, K=K, test=False):\n",
    "        # data = (B, 1, H, W)\n",
    "        B, _, H, W = data.shape\n",
    "\n",
    "        # Generate K copies of each observation. Each will get sampled once according to the generated distribution to generate a total of K observation samples\n",
    "        data_k_vec = data.repeat((1, K, 1, 1)).view(-1, H*W)\n",
    "\n",
    "        # Retrieve the estimated mean and log(standard deviation) estimates from the posterior approximator\n",
    "        mu, logstd = model.encode(data_k_vec)\n",
    "\n",
    "        # Use the reparametrization trick to generate (mean)+(epsilon)*(standard deviation) for each sample of each observation\n",
    "        z = model.reparameterize(mu, logstd)\n",
    "\n",
    "        # Calculate log q(z|x) - how likely are the importance samples given the distribution that generated them?\n",
    "        log_q = compute_log_probabitility_gaussian(z, mu, logstd)\n",
    "\n",
    "        # Calculate log p(z) - how likely are the importance samples under the prior N(0,1) assumption?\n",
    "        log_p_z = compute_log_probabitility_gaussian(z, torch.zeros_like(z, requires_grad=False), torch.zeros_like(z, requires_grad=False))\n",
    "\n",
    "        # Hand the samples to the decoder network and get a reconstruction of each sample.\n",
    "        decoded = model.decode(z)\n",
    "\n",
    "        # Calculate log p(x|z) with a bernoulli distribution - how likely are the recreations given the latents that generated them?\n",
    "        log_p = compute_log_probabitility_bernoulli(decoded, data_k_vec)\n",
    "\n",
    "        # Begin calculating L_alpha depending on the (a) model type, and (b) optimization method\n",
    "        # log_p_z + log_p - log_q = log(p(z_i)p(x|z_i)/q(z_i|x)) = log(p(x,z_i)/q(z_i|x)) = L_VI\n",
    "        #   (for each importance sample i out of K for each observation)\n",
    "\n",
    "        if algorithm in ['iwae','general_alpha'] or test:\n",
    "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
    "            log_w_matrix = (log_p_z + log_p - log_q).view(B, K)*(1-alpha) # <- assumes alpha!=1 !\n",
    "            # Begin using the \"max trick\". Subtract the maximum log(*) sample value for each observation.\n",
    "            # log_w_minus_max = log([p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]))\n",
    "            log_w_minus_max = log_w_matrix - torch.max(log_w_matrix, 1, keepdim=True)[0]\n",
    "            ws_matrix = torch.exp(log_w_minus_max) # <- sum of (sample i / max sample)\n",
    "            # (1/K) SUM (p(x_i,z_i)/q(z_i|x_i))^(1-alpha))\n",
    "            sum_per_datapoint = torch.log(torch.sum(ws_matrix,1))+torch.max(log_w_matrix,1,keepdim=True)[0] - np.log(K)\n",
    "            return -torch.sum(sum_per_datapoint)/(1-alpha)\n",
    "\n",
    "        elif algorithm =='vae':\n",
    "            # Don't reorder, and divide by K in anticipation of taking a batch sum of (1/K)*SUM(log(p(x,z)/q(z|x)))\n",
    "            log_w_matrix = (log_p_z + log_p - log_q).view(B*K, 1)*1/K\n",
    "            return -torch.sum(log_w_matrix)\n",
    "\n",
    "        elif algorithm=='vralpha':\n",
    "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
    "            # Multiply by (1-alpha) because (1-alpha)* log(p(x,z_i)/q(z_i|x)) =  log([p(x,z_i)/q(z_i|x)]^(1-alpha))\n",
    "            log_w_matrix = (log_p_z + log_p - log_q).view(-1, K) * (1-alpha)\n",
    "            log_w_minus_max = log_w_matrix - torch.max(log_w_matrix, 1, keepdim=True)[0]\n",
    "            ws_matrix = torch.exp(log_w_minus_max)\n",
    "            ws_norm = ws_matrix / torch.sum(ws_matrix, 1, keepdim=True)\n",
    "\n",
    "            # If we're specifically using a VR-alpha model, we want to choose a sample to backprop according to the values in ws_norm above\n",
    "            # So we make a distribution in each row\n",
    "            sample_dist = Multinomial(1,ws_norm)\n",
    "            # Then we choose a sample in each row acccording to this distribution\n",
    "            ws_sum_per_datapoint = log_w_matrix.gather(1,sample_dist.sample().argmax(1,keepdim=True))/(1-alpha)\n",
    "\n",
    "            return -torch.sum(ws_sum_per_datapoint)\n",
    "\n",
    "        elif algorithm=='vrmax':\n",
    "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
    "            # Take the max in each row, representing the maximum-weighted sample\n",
    "            log_w_matrix = (log_p_z + log_p - log_q).view(-1, K).max(axis=1,keepdim=True).values\n",
    "            return -torch.sum(log_w_matrix)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "-b0DLZyfrdQk",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Implementation of the VAE used on the MNIST dataset with 2 stochastic layers.\n",
    "class mnist_model_2(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(mnist_model_2, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc31 = nn.Linear(200, 100)  # stochastic 1\n",
    "        self.fc32 = nn.Linear(200, 100)\n",
    "\n",
    "        self.fc4 = nn.Linear(100, 100)\n",
    "        self.fc5 = nn.Linear(100, 100)\n",
    "        self.fc61 = nn.Linear(100, 50)  # Innermost (stochastic 2)\n",
    "        self.fc62 = nn.Linear(100, 50)\n",
    "\n",
    "        self.fc7 = nn.Linear(50, 100)\n",
    "        self.fc8 = nn.Linear(100, 100)\n",
    "        self.fc81 = nn.Linear(100, 100)  # stochastic 1\n",
    "        self.fc82 = nn.Linear(100, 100)\n",
    "\n",
    "        self.fc9 = nn.Linear(100, 200)\n",
    "        self.fc10 = nn.Linear(200, 200)\n",
    "        self.fc11 = nn.Linear(200, 784)  # reconstruction\n",
    "\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.tanh(self.fc1(x))\n",
    "        h2 = torch.tanh(self.fc2(h1))\n",
    "        mu, log_std = self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "        z1 = self.reparameterize(mu, log_std)\n",
    "        h3 = torch.tanh(self.fc4(z1))\n",
    "        h4 = torch.tanh(self.fc5(h3))\n",
    "\n",
    "        return self.fc61(h4), self.fc62(h4), [x, z1]\n",
    "\n",
    "    def reparameterize(self, mu, logstd, test=False):\n",
    "        std = torch.exp(logstd)\n",
    "        if test == True:\n",
    "            eps = torch.zeros_like(mu)\n",
    "        else:\n",
    "            eps = torch.randn_like(std)\n",
    "        # This is the reparametrization trick - represent the sample as a sum rather than black-box generated number\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, test=False):\n",
    "        h5 = torch.tanh(self.fc7(z))\n",
    "        h6 = torch.tanh(self.fc8(h5))\n",
    "        mu, log_std = self.fc81(h6), self.fc82(h6)\n",
    "\n",
    "        z1 = self.reparameterize(mu, log_std, test=test)\n",
    "        h7 = torch.tanh(self.fc9(z1))\n",
    "        h8 = torch.tanh(self.fc10(h7))\n",
    "\n",
    "        return torch.sigmoid(self.fc11(h8))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logstd, _ = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logstd)\n",
    "        return self.decode(z), mu, logstd\n",
    "\n",
    "    def compute_loss_for_batch(self, data, model, K=K, test=False):\n",
    "        B, _, H, W = data.shape\n",
    "\n",
    "        # First repeat the observations K times, representing the data as a flat (M*K, # of pixels)\n",
    "        data_k_vec = data.repeat((1, K, 1, 1)).view(-1, H * W)\n",
    "\n",
    "        # Encode the model and retrieve estimated distribution parameters mu and log(standard deviation) for each sample of each observation\n",
    "        # z1 holds the latent samples generated at the first stochastic layer.\n",
    "        mu, log_std, [x, z1] = self.encode(data_k_vec)\n",
    "\n",
    "        # Sample from each observation's approximated latent distribution in each row (i.e. once for each of K importance samples, represented by rows)\n",
    "        # (this uses the reparametrization trick!)\n",
    "        z = model.reparameterize(mu, log_std)\n",
    "\n",
    "        # Calculate Log p(z) (prior) - how likely are these values given the prior assumption N(0,1)?\n",
    "        log_p_z = torch.sum(-0.5 * z ** 2, 1) - .5 * z.shape[1] * T.log(torch.tensor(2 * np.pi))\n",
    "\n",
    "        # Calculate q (z | h1) - how likely are the generated output latent samples given the distributions they came from?\n",
    "        log_qz_h1 = compute_log_probabitility_gaussian(z, mu, log_std)\n",
    "\n",
    "        # Re-Generate the mu and log_std that generated the first-layer latents z1\n",
    "        h1 = torch.tanh(self.fc1(x))\n",
    "        h2 = torch.tanh(self.fc2(h1))\n",
    "        mu, log_std = self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "        # Calculate log q(h1|x) - how likely are the first-stochastic-layer latents given the distributions they come from?\n",
    "        log_qh1_x = compute_log_probabitility_gaussian(z1, mu, log_std)\n",
    "\n",
    "        # Calculate the distribution parameters that generated the first-layer latents upon decoding\n",
    "        h5 = torch.tanh(self.fc7(z))\n",
    "        h6 = torch.tanh(self.fc8(h5))\n",
    "        mu, log_std = self.fc81(h6), self.fc82(h6)\n",
    "\n",
    "        # Calculate log p(h1|z) - how likely are the latents z1 under the parameters of the distribution here?\n",
    "        #   (This directly encourages the decoder to learn the inverse of the map h1->z)\n",
    "        log_ph1_z = compute_log_probabitility_gaussian(z1, mu, log_std)\n",
    "\n",
    "        # Finally calculate the reconstructed image\n",
    "        h7 = torch.tanh(self.fc9(z1))\n",
    "        h8 = torch.tanh(self.fc10(h7))\n",
    "        decoded = torch.sigmoid(self.fc11(h8))\n",
    "\n",
    "        # calculate log p(x | h1) - how likely is the reconstruction given the latent samples that generated it?\n",
    "        log_px_h1 = compute_log_probabitility_bernoulli(decoded, x)\n",
    "\n",
    "        # Begin calculating L_alpha depending on the (a) model type, and (b) optimization method\n",
    "        # log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x =\n",
    "        #           log([p(z0_i)p(x|z1_i)p(z1_i|z0_i)]/[q(z0_i|z1_i)q(z1_i|x)]) = log(p(x,z0_i,z1_i)/q(z0_i,z1_i|x)) = L_VI\n",
    "        #   (for each importance sample i out of K for each observation)\n",
    "        # Note that if test==True then we're always using the IWAE objective!\n",
    "\n",
    "        if algorithm in ['iwae','general_alpha'] or test:\n",
    "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
    "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(B, K)*(1-alpha)\n",
    "            # Begin using the \"max trick\". Subtract the maximum log(*) sample value for each observation.\n",
    "            # log_w_minus_max = log([p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]))\n",
    "            log_w_minus_max = log_w_matrix - torch.max(log_w_matrix, 1, keepdim=True)[0]\n",
    "            ws_matrix = torch.exp(log_w_minus_max) # <- sum of (sample i / max sample)\n",
    "            # (1/K) SUM (p(x_i,z_i)/q(z_i|x_i))^(1-alpha))\n",
    "            sum_per_datapoint = torch.log(torch.sum(ws_matrix,1))+torch.max(log_w_matrix,1,keepdim=True)[0] - np.log(K)\n",
    "            return -torch.sum(sum_per_datapoint)/(1-alpha)\n",
    "\n",
    "        elif algorithm == 'vae':\n",
    "            # Don't reorder, and divide by K in anticipation of taking a batch sum of (1/K)*SUM(log(p(x,z)/q(z|x)))\n",
    "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, 1) * 1 / K\n",
    "            return -torch.sum(log_w_matrix)\n",
    "\n",
    "        elif algorithm == 'vralpha':\n",
    "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
    "            # Multiply by (1-alpha) because (1-alpha)* log(p(x,z_i)/q(z_i|x)) =  log([p(x,z_i)/q(z_i|x)]^(1-alpha))\n",
    "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, K) * (1 - self.alpha)\n",
    "\n",
    "        elif algorithm == 'vrmax':\n",
    "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
    "            # Take the max in each row, representing the maximum-weighted sample, then immediately return batch sum loss -L_alpha\n",
    "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, K).max(axis=1,keepdim=True).values\n",
    "            return -torch.sum(log_w_matrix)\n",
    "\n",
    "        # Begin using the \"max trick\". Subtract the maximum log(*) sample value for each observation.\n",
    "        # log_w_minus_max = log([p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]))\n",
    "        log_w_minus_max = log_w_matrix - torch.max(log_w_matrix, 1, keepdim=True)[0]\n",
    "\n",
    "        # Exponentiate so that each term is [p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]) (no log)\n",
    "        ws_matrix = torch.exp(log_w_minus_max)\n",
    "\n",
    "        # Calculate normalized weights in each row. Max denominators cancel out!\n",
    "        # ws_norm = [p(z_i,x)/q(z_i|x)]/SUM([p(z_k,x)/q(z_k|x)])\n",
    "        ws_norm = ws_matrix / torch.sum(ws_matrix, 1, keepdim=True)\n",
    "\n",
    "        if algorithm == 'vralpha' and not test:\n",
    "            # If we're specifically using a VR-alpha model, we want to choose a sample to backprop according to the values in ws_norm above\n",
    "            # So we make a distribution in each row\n",
    "            sample_dist = Multinomial(1, ws_norm)\n",
    "\n",
    "            # Then we choose a sample in each row acccording to this distribution\n",
    "            ws_sum_per_datapoint = log_w_matrix.gather(1, sample_dist.sample().argmax(1, keepdim=True))\n",
    "        else:\n",
    "            # For any other model, we're taking the full sum at this point\n",
    "            ws_sum_per_datapoint = torch.sum(log_w_matrix * ws_norm, 1)\n",
    "\n",
    "        if algorithm in [\"general_alpha\", \"vralpha\"] and not test:\n",
    "            # For both VR-alpha and directly estimating L_alpha with a sum, we have to renormalize the sum with 1-alpha\n",
    "            ws_sum_per_datapoint /= (1 - alpha)\n",
    "\n",
    "        loss = -torch.sum(ws_sum_per_datapoint)\n",
    "\n",
    "        return loss"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KVDo9bU_rdQm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Compute N(obs| mu, sigma) for all K samples and sum over probabilities of the K samples\n",
    "def compute_log_probabitility_gaussian(obs, mu, logstd, axis=1):\n",
    "    return torch.sum(-0.5 * ((obs-mu) / torch.exp(logstd)) ** 2 - logstd, axis)-.5*obs.shape[1]*T.log(torch.tensor(2*np.pi))\n",
    "\n",
    "# Compute Ber(obs| theta) for all K samples and sum over probabilities of the K samples\n",
    "def compute_log_probabitility_bernoulli(theta, obs, axis=1):\n",
    "    # 1e-18 needed to avoid numerical errors\n",
    "    return torch.sum(obs*torch.log(theta+1e-18) + (1-obs)*torch.log(1-theta+1e-18), axis)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "khnN29nKrdQp",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# train and test functions\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        # (B, 1, F1, F2) (e.g. (128, 1, 28, 28) for MNIST with B=128)\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model.compute_loss_for_batch(data, model)\n",
    "        # comment this back in in case of NaNs\n",
    "        #with detect_anomaly():\n",
    "        #    loss.backward()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "        logging.info(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = model.compute_loss_for_batch(data, model, K=5000,test=True)\n",
    "            test_loss += loss.item()\n",
    "            if i == 0:\n",
    "                # Visualizing reconstructions\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(test_batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         f'results/reconstruction_{algorithm}_L={L}_{data_name}_alpha={alpha}_K={K}_epoch={epoch}.png',\n",
    "                           nrow=n)\n",
    "                # Visualizing random samples from the latent space\n",
    "                noise = torch.randn(64, 50).to(device)\n",
    "                sample = model.decode(noise).cpu()\n",
    "                save_image(sample.view(64, 1, 28, 28),\n",
    "                           f'results/sample_{algorithm}_L={L}_{data_name}_alpha={alpha}_K={K}_epoch={epoch}.png')\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Test set loss: {test_loss:.4f}')\n",
    "    logging.info(f'====> Epoch: {epoch} Test set loss: {test_loss:.4f}')\n",
    "    return test_loss"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "b-wqHyVxrdQs",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386,
     "referenced_widgets": [
      "dfccf3e89c5e48f78dbf28ad805b61fb",
      "8e33bf64cd0e4d2d98941e0f0275fc24",
      "09781824cedf4b42a66e01c71fa81193",
      "3dd98068324045c799882059450f86c3",
      "030207cfadba4ef2a5c60504b76129b0",
      "e0ea5542504a4115baf1177cb52829c2",
      "f3836cff1fff4c009fe020c4c5db6415",
      "d3d99c89e39f40f39a6e9af6994c2f78",
      "aec2134630f44df28903c79479ca0589",
      "7858d1bc512b4858b9d5b7d483cc6190",
      "04afe340d0384819b7a0e921fda29306",
      "db288bf9fdc04a7ba4a17c0d2753f4c1",
      "648128391d264fcf8b73f595ad181916",
      "e66dd1c3d27c4318952324d8b8c7ff30",
      "b513d76c07b64f969c7477971d5a5c4e",
      "6c2807ba2c154e93903d6641af445714",
      "4511f97522a34cb9b1a987ef8baf444d",
      "8a355c168662472fa78dbb82b2da7f9a",
      "61e0b1f23a2146e39330c6ff239c74a7",
      "9891dc7983b140ba9c6acd6f94bc748b",
      "04eec0229cfc420980834c43a87cd996",
      "3388ec542ea64e8c8890635808a62beb",
      "c7833054c33b4292a3e03602f80921ce",
      "eaf9eb1c701a43e2bd5e49cac250ff71",
      "ceba0c9fb6544a0380bd6e9705179eeb",
      "c6ca8194cb254960ba42f4f6c90851a6",
      "9ecb94f9a4864b1497870728bd58fb36",
      "460be2634f574eb19fed39bf6703394a",
      "24fcc7830ad94dddbe0eb78ef0a181eb",
      "bea6c4ed9d0d42e9b3e62e7d67ffb300",
      "4cd338ecc21148fa8ad795f766668ec8",
      "e5bd7c8bd8a34a07aab01ae5d0cb4728"
     ]
    },
    "outputId": "21eeb090-28ad-459d-a66a-09e9530960e3"
   },
   "source": [
    "if L==1:\n",
    "    model = mnist_model_1(alpha).to(device)\n",
    "else:\n",
    "    model = mnist_model_2(alpha).to(device)\n",
    "train_loader, test_loader = load_data_and_initialize_loaders(data_name, train_batch_size, test_batch_size)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfccf3e89c5e48f78dbf28ad805b61fb",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec2134630f44df28903c79479ca0589",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4511f97522a34cb9b1a987ef8baf444d",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceba0c9fb6544a0380bd6e9705179eeb",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Training on GPU\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Training on GPU\")\n",
    "    logging.info(\"Training on GPU\")\n",
    "\n",
    "print(f'{datetime.datetime.now()} \\nStarting training')\n",
    "logging.info(f'{datetime.datetime.now()} \\nStarting training')\n",
    "for e in range(1, epochs+1):\n",
    "    train(e)\n",
    "    if e % test_interval == 0:\n",
    "        test(e)\n",
    "test(epochs)\n",
    "print(datetime.datetime.now())\n",
    "logging.info(datetime.datetime.now())\n",
    "print(\"Training finished\")\n",
    "logging.info(\"Training finished\")\n",
    "\n",
    "print(\"Saving model\")\n",
    "torch.save(model.state_dict(),\n",
    "           f'models/{algorithm}_L={L}_{data_name}_alpha={alpha}_K={K}_epochs={epochs}.pt')\n",
    "print(\"Saved model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IEaIzm1crdQu",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "outputId": "5d374c62-6d82-453e-dcbd-3992b099a3b3"
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Training on GPU\")\n",
    "    logging.info(\"Training on GPU\")\n",
    "\n",
    "print(f'{datetime.datetime.now()} \\nStarting training')\n",
    "logging.info(f'{datetime.datetime.now()} \\nStarting training')\n",
    "for e in range(1, epochs+1):\n",
    "    train(e)\n",
    "    if e % test_interval == 0:\n",
    "        test(e)\n",
    "test(epochs)\n",
    "print(datetime.datetime.now())\n",
    "logging.info(datetime.datetime.now())\n",
    "print(\"Training finished\")\n",
    "logging.info(\"Training finished\")\n",
    "\n",
    "print(\"Saving model\")\n",
    "torch.save(model.state_dict(),\n",
    "           f'models/{algorithm}_L={L}_{data_name}_alpha={alpha}_K={K}_epochs={epochs}.pt')\n",
    "print(\"Saved model\")"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Training on GPU\n",
      "2020-06-12 18:12:56.008404 \n",
      "Starting training\n",
      "====> Epoch: 1 Average loss: 237.5309\n",
      "====> Epoch: 1 Test set loss: 234.6304\n",
      "2020-06-12 18:13:33.955103\n",
      "Training finished\n",
      "Saving model\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}